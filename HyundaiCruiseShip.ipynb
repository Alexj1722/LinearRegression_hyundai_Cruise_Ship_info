{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004f4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos las importaciones necesarias para empezar\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb1e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/31 21:15:09 WARN Utils: Your hostname, black19hunter resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "21/10/31 21:15:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/alex1722/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/31 21:15:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Tarea').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79dcc55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Cargamos el archivo que nos funcionara como la base de datos\n",
    "df=spark.read.csv(\"hyundai cruise ship info.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c85b033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ship_name: string (nullable = true)\n",
      " |-- Cruise_line: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tonnage: double (nullable = true)\n",
      " |-- passengers: double (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- cabins: double (nullable = true)\n",
      " |-- passenger_density: double (nullable = true)\n",
      " |-- crew: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema nos muestra la informacion general sobre las columnas existentes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b53082d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|Ship_name|Cruise_line|               Age|           Tonnage|       passengers|           length|            cabins|passenger_density|             crew|\n",
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|      158|        158|               158|               158|              158|              158|               158|              158|              158|\n",
      "|   mean| Infinity|       null|15.689873417721518| 71.28467088607599|18.45740506329114|8.130632911392404| 8.830000000000005|39.90094936708861|7.794177215189873|\n",
      "| stddev|     null|       null| 7.615691058751413|37.229540025907866|9.677094775143416|1.793473548054825|4.4714172221480615| 8.63921711391542|3.503486564627034|\n",
      "|    min|Adventure|    Azamara|                 4|             2.329|             0.66|             2.79|              0.33|             17.7|             0.59|\n",
      "|    max|Zuiderdam|   Windstar|                48|             220.0|             54.0|            11.82|              27.0|            71.43|             21.0|\n",
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#esta linea nos muestra un resumen de los datos del dataframe\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13a3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos StringIndexer\n",
    "#StringIndexer asigna una columna de cadenas de etiquetas a una columna ML de índices de etiquetas. \n",
    "#Si la columna de entrada es numérica, la convertimos en una cadena e indexamos los valores de la cadena.\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65c986ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Ship_name='Journey', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55, cruise_category=16.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aqui convertimos a cadena e indexamos los valores\n",
    "indexer = StringIndexer(inputCol = 'Cruise_line', outputCol = 'cruise_category')\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678458f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density',\n",
       " 'crew',\n",
       " 'cruise_category']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aqui vemos las columnas registradas\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77beaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos mas librerias necesarias\n",
    "#VectorAssembler:Un transformador de características que fusiona varias columnas en una columna vectorial.\n",
    "#Vectors:Métodos de fábrica para trabajar con vectores.\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad1a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui se seleccionan las columnas que seran fusionadas y se define la columna de salida que contendra\n",
    "#la fucion de todas las columnas y la variable o dataset que contendra esos valores\n",
    "assembler = VectorAssembler(inputCols = ['Age',\n",
    " 'Tonnage',\n",
    " 'passengers',\n",
    " 'length',\n",
    " 'cabins',\n",
    " 'passenger_density',\n",
    " 'cruise_category'], outputCol = 'features')\n",
    "\n",
    "#en la instruccion assembler.transform se transforma el conjunto de datos de entrada con parámetros opcionales.\n",
    "output = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da097b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5af090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density',\n",
       " 'crew',\n",
       " 'cruise_category',\n",
       " 'features']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aqui se muestran las columnas existentes\n",
    "#se puede observar que aparece la columna creada anteriormente llamada features\n",
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db4b6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|crew|\n",
      "+--------------------+----+\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[26.0,47.262,14.8...| 6.7|\n",
      "|[11.0,110.0,29.74...|19.1|\n",
      "|[17.0,101.353,26....|10.0|\n",
      "|[22.0,70.367,20.5...| 9.2|\n",
      "|[15.0,70.367,20.5...| 9.2|\n",
      "|[23.0,70.367,20.5...| 9.2|\n",
      "|[19.0,70.367,20.5...| 9.2|\n",
      "|[6.0,110.23899999...|11.5|\n",
      "|[10.0,110.0,29.74...|11.6|\n",
      "|[28.0,46.052,14.5...| 6.6|\n",
      "|[18.0,70.367,20.5...| 9.2|\n",
      "|[17.0,70.367,20.5...| 9.2|\n",
      "|[11.0,86.0,21.24,...| 9.3|\n",
      "|[8.0,110.0,29.74,...|11.6|\n",
      "|[9.0,88.5,21.24,9...|10.3|\n",
      "|[15.0,70.367,20.5...| 9.2|\n",
      "|[12.0,88.5,21.24,...| 9.3|\n",
      "|[20.0,70.367,20.5...| 9.2|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#este data set contendra solo 2 columnas las cuales son seleccionadas\n",
    "final_data = output.select(['features','crew'])\n",
    "output.select(['features', 'crew']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61d926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos LinearRegression\n",
    "#LinearRegression: El objetivo de aprendizaje es minimizar la función de pérdida especificada, \n",
    "#con regularización. Esto admite dos tipos de pérdida:\n",
    "#1-squareError (también conocido como pérdida al cuadrado)\n",
    "#huber (un híbrido de error cuadrado para errores relativamente pequeños y error absoluto para errores \n",
    "#relativamente grandes, y estimamos el parámetro de escala a partir de los datos de entrenamiento)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe152c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos columna para regreson linear\n",
    "model = LinearRegression(labelCol= 'crew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5c3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cf25eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              crew|\n",
      "+-------+------------------+\n",
      "|  count|               104|\n",
      "|   mean|7.8663461538461625|\n",
      "| stddev| 3.340824285732549|\n",
      "|    min|              0.88|\n",
      "|    max|              19.1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aqui se prueba el modelo antes de ser entrenado\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3], seed = 1234)\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3301599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/31 21:27:58 WARN Instrumentation: [19fe2429] regParam is zero, which might cause numerical instability and overfitting.\n",
      "21/10/31 21:27:59 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "21/10/31 21:27:59 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "21/10/31 21:27:59 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "#aqui se entrena el modelo\n",
    "trained_model = model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2bab1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#asignamos los resultados de evaluacion del modelo entrenado\n",
    "results = trained_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29fbb542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex1722/.local/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  0.3907617507368393|\n",
      "| -1.3231760234284078|\n",
      "|-0.00470018105311...|\n",
      "| 0.42353817560602103|\n",
      "|  -0.590615509738539|\n",
      "| -0.5270445767004297|\n",
      "| -0.4381410108920214|\n",
      "|  1.0988796006491341|\n",
      "|-0.09517679473458429|\n",
      "|-0.38095805561306406|\n",
      "|  -0.318811648310362|\n",
      "| -0.3169629832857588|\n",
      "|  0.9235881855682031|\n",
      "|   0.714761719631607|\n",
      "|   0.079251944729961|\n",
      "|-0.49423686557798874|\n",
      "|  0.8046967352123495|\n",
      "|   0.796610479216028|\n",
      "| 0.22168740717856217|\n",
      "| -0.9818072806326583|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mostramos resultados\n",
    "results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b3d1395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5772151741728508\n",
      "MAE: 0.4648120224953151\n",
      "R2: 0.9768121314673073\n"
     ]
    }
   ],
   "source": [
    "#perdida, marcador, error cuadratico medio, error absoluto medio\n",
    "print('RMSE: {}'.format(results.rootMeanSquaredError))\n",
    "print('MAE: {}'.format(results.meanAbsoluteError))\n",
    "print('R2: {}'.format(results.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bc66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf2409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd686897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75681b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6426689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ad08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
